---
title: "What do the facebook or x say about farming communities?"
subtitle: "A word cloud analysis using R"
author: "Shuyu Zhang"
co-author: "Claudiu Forgaci"
date: "20 March 2024"
format: html
toc: true
include-in-header:
  - text: |
      <style>
      .cell-output-stdout code {
        word-break: break-wor !important;
        white-space: pre-wrap !important;
      }
      </style>
---

::: callout-note
After analyzing the dutch news data, if you still have interests in working social media data and to hear more about citizen voice, you can follow the documents to do the very similar things. If you are reading the source document `script_socialmedia.qmd` and want to see a rendered version of it, click on the Render button above this window.
:::

## Setup

### Software of collecting data

1.  During the workshop, we work with [Web Data Research Assistant](https://southampton.ac.uk/~lac/WebDataResearchAssistant/) for collecting Facebook and X data, it is a web browser extension and using Web sourced data, in particular Twitter, Facebook, Google (not work for google map), and Instagram.

2.  Install Web data research assistant in chrome: [Install](https://chromewebstore.google.com/detail/web-data-research-assista/kcdbekkmigohaijilebpaeoopcgjbbdm?pli=1)

3.  Go to facebook or x and search for keywords you are interested, then Press Shift-Ctrl-A (the Shift key + Control key + the A key) to activate the data gathering process. ![](fig/webscraping1.png)

4.  Press Shift-Ctrl-H (the Shift key + Control key + the H key) to halt the data gathering process and stop the browser scrolling. A webdatara.html will show in the bottom of the page. ![](fig/webscraping2.png) 5.Open the webdatara.html, copy all the data, open a new excel and paste all the data into the excel file. ![](fig/webscraping3.png)

### Software of analysing data

1.  During the workshop, we work in [a cloud instance of RStudio](http://rstudio-server-edu.bk.tudelft.nl:8787/) accessible through your web browser, so no installation is required. After the workshop, you will need to install [R, RStudio Desktop](https://posit.co/download/rstudio-desktop/) and [Python](https://www.python.org/downloads/) for your operating system.

2.  In RStudio, create a new project from `File > New Project... > Version Control > Git` with the URL `https://github.com/cforgaci/mint2324Q3U.git` and project directory name `mint2324Q3U`. Browse to a location of your choice on your computer and click on `Create Project`. This will create a project directory populated with the data scripts used in the workshop.

3.  Open `script.qmd`. This will bring you to the computational notebook from where this document was rendered. Activate the visual editor and continue reading there. ![](fig/rstudio.png)

4.  For our analysis, we will need to load a number of R packages that extend the out-of-the-box functionality of R. Run the code chunk below by pressing on the green arrow in its upper right corner. If this is the first time you run the script, it will take a couple of minutes until all packages are installed.

```{r setup, message=FALSE}
# Install packages
if (!"renv" %in% installed.packages()) install.packages("renv")
renv::restore(prompt = FALSE)

# Load packages
library(tidyverse)        # Data manipulation and visualisation
library(reticulate)       # R interface with Python
library(tidytext)         # Text manipulation
library(topicmodels)      # Topic modeling
library(LDAvis)           # Interactive visualisation of LDA models
require(servr)            # LDAvis dependency
library(wordcloud)        # Generate word clouds
require(reshape2)         # wordcloud dependency
library(openai)           # Access ChatGPT from R
library(googleLanguageR)  # Text translation with Google Translation API
library(deeplr)           # Text translation with DeepL API
library(readxl)           # Read Excel files in R
library(stopwords)        # Provides stopwords lists for multiple languages
library(tm)               # Create and manipulate corpora
```

## Introduction

In this workshop, we will use word clouds to reveal top keywords in the facebook or x about farmer communities.

### The dataset

We will do this together on a given dataset: two data frames of excel files which generated by Web Data Research Assistant and translated by Google. The one is from facebook and another is from x. In this instruction, we will work with data frames of social media x. After that, you will run the analysis in this document on a dataset of your choice. Run the following code chunk with the default value.

```{r}
# What data will you work with? Use one of the following two values:
# - "facebook_x" if you want to use the default dataset
# - "mydata" if you want to use your own dataset
data_choice <- "facebook_x"
```

## Text analysis

Read the excel files of facebook or x in R, we use excel file of x as an example.

```{r readxl}
# read the excel data from folder, using data from x for example
data_root <- paste0("data/", data_choice, "/", "x#boeren_nederland_EN.xlsx")
fsq <- read_excel(data_root)
```

At this step, we define a list of stop words that occur in high frequency across the entire set of documents and are not expected to add meaning to the topics.

```{r stopwords}
stopwords_vec <- stopwords()
custom_stopwords_vec <- c(stopwords_vec,"https", "twitter", "x", "also", "think", "must", "know", "many", "much", "like", "everything", "really", "want","without", "going", "good", "something", "farmers", "netherlands", "nthe" )
```

We choose the column of text information, split the entire corpus into words and filter them with some rules...

```{r tokens, warning=FALSE}
fsq_words <- fsq %>% 
  unnest_tokens(output = token, input = Text) %>%  # Decide which text information in which columns you want to analyze, and input = "name of column"
  filter(!token %in% stopwords_vec) %>%  # Remove English stop words
  filter(!token %in% custom_stopwords_vec) %>%  # Remove custom stop words
  filter(!str_detect(token, "[0-9]")) %>%  # Remove numbers
  filter(nchar(token) > 3)  # Remove words of max. 3 characters
```

... and have a quick look at the most frequently used words. We can already guess what this set of boeron of netherlands on social media x are about: frequently used words such as "people", "food", and "policy" "right" indicate different areas of concern present in social media posts. Note that we have 1609 rows, each representing one distinct word. This is the vocabulary we will provide as input to the word cloud.

```{r tokens-rank}
# Count the token frequency
fsq_words %>% 
  count(token) %>% 
  arrange(desc(n))
```

We select top 100 words and make a word cloud

```{r wordcloud}
# Select top 100 words
fsq_words_top100 <- fsq_words %>% 
  count(token) %>%  
  slice_max(n, n = 100) 

# Generate a word cloud with the filtered data
wordcloud(words = fsq_words_top100$token, freq = fsq_words_top100$n, 
          colors = rainbow(length(fsq_words_top100$token)), scale=c(3, 0.2))
```
